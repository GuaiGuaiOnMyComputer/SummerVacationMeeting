{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Using Different Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial expansion of a function $f$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "f(x) = \\sum^{N}_{k=0}{w_k x^k} = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + w_5x^5 .....\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "In this demonstration, we attempt to calculate the coefficients $w_k$ in the polynomial expansion of $sin(x)$. By gradient descent, the optimal value for each weight $w_k$ can be found when \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_k} \\approx 0\n",
    "\\end{equation}\n",
    "\n",
    "And the loss function $L$ is the $L_2$ loss of our approximation, defined as\n",
    "\n",
    "\\begin{equation}\n",
    "L = (\\hat{y} - y)^2 \\quad \\text{$\\hat{y}$: true value of $sin(x)$}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the partial derivative of $L$ with respect to each weight, the gradient can be found\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial (\\hat{y}-y)^2}{\\partial w_k} = 2(\\hat{y}-y)x^k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html# \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-np.pi, np.pi, N_SAMPLES)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "np.random.seed(0)\n",
    "w = np.random.random(8)\n",
    "\n",
    "for t in range(4000):\n",
    "\n",
    "    y_pred = w[0] + w[1]*x + w[2]*(x**2) + w[3]*(x**3) + w[4]*(x**4) + w[5]*(x**5) + w[6]*(x**6) + w[7]*(x**7)\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = (error ** 2).mean()\n",
    "    gradients = np.array((\n",
    "        2 * error,\n",
    "        2 * error * x,\n",
    "        2 * error * (x**2),\n",
    "        2 * error * (x**3),\n",
    "        2 * error * (x**4),\n",
    "        2 * error * (x**5),\n",
    "        2 * error * (x**6),\n",
    "        2 * error * (x**7)\n",
    "    )).mean(-1)\n",
    "    w -= gradients * LEARNING_RATE\n",
    "    \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, N_SAMPLES, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "np.random.seed(0)\n",
    "w = torch.randn(8, device = device, dtype = dtype)\n",
    "\n",
    "for t in range(4000):\n",
    "    y_pred = w[0] + w[1]*x + w[2]*x**2 + w[3]*x**3 + w[4]*x**4 + w[5]*x**5 + w[6]*x**6 + w[7]*x**7\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = error.pow(2).mean().item()\n",
    "\n",
    "    gradients = torch.empty_like(w, device = device)\n",
    "    gradients[0] = (2 * error).mean(-1)\n",
    "    gradients[1] = (2 * error * x).mean(-1)\n",
    "    gradients[2] = (2 * error * (x**2)).mean(-1)\n",
    "    gradients[3] = (2 * error * (x**3)).mean(-1)\n",
    "    gradients[4] = (2 * error * (x**4)).mean(-1)\n",
    "    gradients[5] = (2 * error * (x**5)).mean(-1)\n",
    "    gradients[6] = (2 * error * (x**6)).mean(-1)\n",
    "    gradients[7] = (2 * error * (x**7)).mean(-1)\n",
    "\n",
    "    \"\"\" \n",
    "    ValueError: only tensor with size 1 can be converted to python scalar\n",
    "    gradients = torch.tensor((\n",
    "        2 * error,\n",
    "        2 * error * x,\n",
    "        2 * error * (x**2),\n",
    "        2 * error * (x**3),\n",
    "        2 * error * (x**4),\n",
    "        2 * error * (x**5),\n",
    "        2 * error * (x**6),\n",
    "        2 * error * (x**7)), device = device).mean(-1)\n",
    "    gradients = torch.from_numpy(__gradients, device = device)\n",
    "    \"\"\"\n",
    "\n",
    "    w -= gradients * LEARNING_RATE\n",
    "    \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, N_SAMPLES, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "torch.manual_seed(0)\n",
    "w = torch.randn(8, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "for t in range(4000):\n",
    "    y_pred = w[0] + w[1]*x + w[2]*x**2 + w[3]*x** 3 + w[4]*x**4 + w[5]*x**5 + w[6]*x**6 + w[7]*x**7\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = error.pow(2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * LEARNING_RATE\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad[:] = 0\n",
    "        \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Defining new autograd functions\n",
    "\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "In this example we define our model as y=a+bP3(c+dx)y=a+bP3​(c+dx) instead of y=a+bx+cx2+dx3y=a+bx+cx2+dx3, where P3(x)=12(5x3−3x)P3​(x)=21​(5x3−3x) is the Legendre polynomial of degree three. We write our own custom autograd function for computing forward and backward of P3P3​, and use it to implement our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Defining new autograd functions\n",
    "\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "In this example we define our model as y=a+bP3(c+dx)y=a+bP3​(c+dx) instead of y=a+bx+cx2+dx3y=a+bx+cx2+dx3, where P3(x)=12(5x3−3x)P3​(x)=21​(5x3−3x) is the Legendre polynomial of degree three. We write our own custom autograd function for computing forward and backward of P3P3​, and use it to implement our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
