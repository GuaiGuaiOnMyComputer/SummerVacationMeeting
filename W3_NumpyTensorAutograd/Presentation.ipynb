{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Using Different Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Taylor series expansion of a function $f$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "f(x) = \\sum^N_{k=0}{\\frac {f^{(k)}(x-a)}{n!}(x-a)^k}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "And the Taylor series expansion of sine function at $a=0$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "sin(x) = \\sum^{N}_{k=0}{\\frac{(-1)^{k}}{(2k+1)!} x^{2k+1}} = \\frac{x^1}{1!} - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!}.....\n",
    "\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we somehow are not able to calculate the Taylor coefficients, which are the $\\frac {(-1)^k}{(2k+1)!}$ terms, replacing the coefficients in the equation with unknown weights $w_{k}$, we can still calcualte them using the gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y} = \\sum^{N}_{k=0}{w_{k}x^{2k+1}} = w_0 x^1 + w_1 x^3 + w_2 x^5 + w_3 x^7...\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><image src=\"img/TaylorSeriesNeuralNetwork.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use $L_2$ loss as the loss function, defined as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Loss function } L = \\sqrt{(\\hat{y} - y)^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of a weight $w_i$ can be determined by the following formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_i} &= \\frac {1}{2} [(\\hat{y} - y)^2]^{\\frac {-1}{2}}  2(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial w_i} = \\frac{\\partial \\hat{y}}{\\partial w_i} \\\\\n",
    "\\frac{\\partial L}{\\partial w_0} &= x \\\\\n",
    "\\frac{\\partial L}{\\partial w_1} &= x^3 \\\\\n",
    "\\frac{\\partial L}{\\partial w_2} &= x^5 \\\\\n",
    "\\frac{\\partial L}{\\partial w_3} &= x^7 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  9.34317819718455e+213\n",
      "loss:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\PyTorchEnv\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\yusin\\AppData\\Local\\Temp\\ipykernel_10508\\682617672.py:21: RuntimeWarning: overflow encountered in square\n",
      "  loss = (error ** 2).mean()\n",
      "c:\\ProgramData\\Miniconda3\\envs\\PyTorchEnv\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n",
      "loss:  nan\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html# \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "EPOCHES = 4000\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-np.pi, np.pi, N_SAMPLES)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w = np.random.random(8)\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    y_pred = w[0] + w[1]*x + w[2]*(x**2) + w[3]*(x**3) + w[4]*(x**4) + w[5]*(x**5) + w[6]*(x**6) + w[7]*(x**7)\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = (error ** 2).mean()\n",
    "    gradients = np.array((\n",
    "        2 * error,\n",
    "        2 * error * x,\n",
    "        2 * error * (x**2),\n",
    "        2 * error * (x**3),\n",
    "        2 * error * (x**4),\n",
    "        2 * error * (x**5),\n",
    "        2 * error * (x**6),\n",
    "        2 * error * (x**7)\n",
    "    )).mean(-1)\n",
    "    w -= gradients * LEARNING_RATE\n",
    "    if t % 100 == 99:\n",
    "        print(\"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "EPOCHES = 10\n",
    "BATCH_SIZE = 5\n",
    "ITERATIONS = int(N_SAMPLES / BATCH_SIZE)\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, N_SAMPLES, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "w = torch.randn(8, device = device, dtype = dtype)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = w[0] + w[1]*x + w[2]*x**2 + w[3]*x**3 + w[4]*x**4 + w[5]*x**5 + w[6]*x**6 + w[7]*x**7\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w[1] -= LEARNING_RATE * grad_a\n",
    "    w[1] -= LEARNING_RATE * grad_b\n",
    "    w[1] -= LEARNING_RATE * grad_c\n",
    "    w[1] -= LEARNING_RATE * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 456.5595922183849\n",
      "199 315.4104678017535\n",
      "299 219.0011175503335\n",
      "399 153.07271311223187\n",
      "499 107.93648186418463\n",
      "599 76.99960154421294\n",
      "699 55.7708589044827\n",
      "799 41.187234405293395\n",
      "899 31.157349632267845\n",
      "999 24.251606046242276\n",
      "1099 19.49165216381009\n",
      "1199 16.20717963733107\n",
      "1299 13.938406765232727\n",
      "1399 12.369598078338889\n",
      "1499 11.283690531878376\n",
      "1599 10.531289710703202\n",
      "1699 10.009460658061258\n",
      "1799 9.647202354268897\n",
      "1899 9.395488211489502\n",
      "1999 9.220429484460588\n",
      "Result: y = -0.0188733346205499 + 0.8477607747912781 x + 0.0032559642756885367 x^2 + -0.09205305252653492 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
