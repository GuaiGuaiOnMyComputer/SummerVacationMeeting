{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Using Different Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial expansion of a function $f$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "f(x) = \\sum^{N}_{k=0}{w_k x^k} = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + w_5x^5 .....\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "In this demonstration, we attempt to calculate the coefficients $w_k$ in the polynomial expansion of $sin(x)$. By gradient descent, the optimal value for each weight $w_k$ can be found when \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_k} \\approx 0\n",
    "\\end{equation}\n",
    "\n",
    "And the loss function $L$ is the $L_2$ loss of our approximation, defined as\n",
    "\n",
    "\\begin{equation}\n",
    "L = (\\hat{y} - y)^2 \\quad \\text{$\\hat{y}$: true value of $sin(x)$}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the partial derivative of $L$ with respect to each weight, the gradient can be found\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{\\partial (\\hat{y}-y)^2}{\\partial w_k} = 2(\\hat{y}-y)x^k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><image src=\"img/TaylorSeriesNeuralNetwork.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at   200 iteration:  98.422\n",
      "Loss at   400 iteration:  84.372\n",
      "Loss at   600 iteration:  72.518\n",
      "Loss at   800 iteration:  62.510\n",
      "Loss at  1000 iteration:  54.054\n",
      "Loss at  1200 iteration:  46.905\n",
      "Loss at  1400 iteration:  40.854\n",
      "Loss at  1600 iteration:  35.728\n",
      "Loss at  1800 iteration:  31.380\n",
      "Loss at  2000 iteration:  27.688\n",
      "Loss at  2200 iteration:  24.547\n",
      "Loss at  2400 iteration:  21.871\n",
      "Loss at  2600 iteration:  19.586\n",
      "Loss at  2800 iteration:  17.632\n",
      "Loss at  3000 iteration:  15.956\n",
      "Loss at  3200 iteration:  14.516\n",
      "Loss at  3400 iteration:  13.273\n",
      "Loss at  3600 iteration:  12.199\n",
      "Loss at  3800 iteration:  11.266\n",
      "Loss at  4000 iteration:  10.453\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://pytorch.org/tutorials/beginner/pytorch_with_examples.html# \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-np.pi, np.pi, N_SAMPLES)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "np.random.seed(0)\n",
    "w = np.random.random(8)\n",
    "\n",
    "for t in range(4000):\n",
    "\n",
    "    y_pred = w[0] + w[1]*x + w[2]*(x**2) + w[3]*(x**3) + w[4]*(x**4) + w[5]*(x**5) + w[6]*(x**6) + w[7]*(x**7)\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = (error ** 2).mean()\n",
    "    gradients = np.array((\n",
    "        2 * error,\n",
    "        2 * error * x,\n",
    "        2 * error * (x**2),\n",
    "        2 * error * (x**3),\n",
    "        2 * error * (x**4),\n",
    "        2 * error * (x**5),\n",
    "        2 * error * (x**6),\n",
    "        2 * error * (x**7)\n",
    "    )).mean(-1)\n",
    "    w -= gradients * LEARNING_RATE\n",
    "    \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
=======
   "execution_count": 12,
>>>>>>> CtrlCCtrlV
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 2.6759906724882334e-05\n",
      "Iteration 200 loss: 0.0001252901113645463\n",
      "Iteration 300 loss: 0.0003584273820910932\n",
      "Iteration 400 loss: 0.000859314584315562\n",
      "Iteration 500 loss: 0.0018797890075463966\n",
      "Iteration 600 loss: 0.003872087887272713\n",
      "Iteration 700 loss: 0.007623983495333534\n",
      "Iteration 800 loss: 0.014483409758998972\n",
      "Iteration 900 loss: 0.026735221047669927\n",
      "Iteration 1000 loss: 0.04823659670783413\n",
      "Iteration 1100 loss: 0.08548941566164665\n",
      "Iteration 1200 loss: 0.1494442939032218\n",
      "Iteration 1300 loss: 0.2585171448006268\n",
      "Iteration 1400 loss: 0.4435923509672864\n",
      "Iteration 1500 loss: 0.7562404950379013\n",
      "Iteration 1600 loss: 1.282068265134208\n",
      "Iteration 1700 loss: 2.1621469362028654\n",
      "Iteration 1800 loss: 3.626973131183707\n",
      "Iteration 1900 loss: 6.049586646011822\n",
      "Iteration 2000 loss: 10.02754817843819\n",
      "Iteration 2100 loss: 16.50778113638307\n",
      "Iteration 2200 loss: 26.974215153486252\n",
      "Iteration 2300 loss: 43.72626134391463\n",
      "Iteration 2400 loss: 70.28707970403683\n",
      "Iteration 2500 loss: 111.99524550516824\n",
      "Iteration 2600 loss: 176.85292666705433\n",
      "Iteration 2700 loss: 276.72954786407473\n",
      "Iteration 2800 loss: 429.05412907632825\n",
      "Iteration 2900 loss: 659.1747158205407\n",
      "Iteration 3000 loss: 1003.6231880842399\n",
      "Iteration 3100 loss: 1514.6032147881538\n",
      "Iteration 3200 loss: 2266.125093284032\n",
      "Iteration 3300 loss: 3362.3532876366257\n",
      "Iteration 3400 loss: 4948.9241546865715\n",
      "Iteration 3500 loss: 7228.251681238243\n",
      "Iteration 3600 loss: 10480.195052132507\n",
      "Iteration 3700 loss: 15089.951888936823\n",
      "Iteration 3800 loss: 21585.719648370552\n",
      "Iteration 3900 loss: 30689.612798627568\n",
      "Iteration 4000 loss: 43386.64613783312\n",
      "Iteration 4100 loss: 61018.453836754335\n",
      "Iteration 4200 loss: 85411.03681602086\n",
      "Iteration 4300 loss: 119049.54365216018\n",
      "Iteration 4400 loss: 165318.35965927457\n",
      "Iteration 4500 loss: 228832.2762723632\n",
      "Iteration 4600 loss: 315895.2032244356\n",
      "Iteration 4700 loss: 435138.1573760056\n",
      "Iteration 4800 loss: 598410.1123178457\n",
      "Iteration 4900 loss: 822026.597752001\n",
      "Iteration 5000 loss: 1128525.8349608486\n",
      "Iteration 5100 loss: 1549146.6334512273\n",
      "Iteration 5200 loss: 2127334.7966990625\n",
      "Iteration 5300 loss: 2923717.63814877\n",
      "Iteration 5400 loss: 4023176.911627123\n",
      "Iteration 5500 loss: 5544924.014533576\n",
      "Iteration 5600 loss: 7656873.30684164\n",
      "Iteration 5700 loss: 10596170.28048635\n",
      "Iteration 5800 loss: 14698532.484453497\n",
      "Iteration 5900 loss: 20440203.122254264\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.001574443422692535\n",
      "Iteration 200 loss: 0.006525394778985337\n",
      "Iteration 300 loss: 0.015569941186179376\n",
      "Iteration 400 loss: 0.03002495148786148\n",
      "Iteration 500 loss: 0.05201844552208236\n",
      "Iteration 600 loss: 0.08485082307534962\n",
      "Iteration 700 loss: 0.1335910953378323\n",
      "Iteration 800 loss: 0.2060509523157666\n",
      "Iteration 900 loss: 0.3143724136084331\n",
      "Iteration 1000 loss: 0.4776148901486508\n",
      "Iteration 1100 loss: 0.7259690423982362\n",
      "Iteration 1200 loss: 1.1076103391153784\n",
      "Iteration 1300 loss: 1.6998129507806952\n",
      "Iteration 1400 loss: 2.62688825696771\n",
      "Iteration 1500 loss: 4.088953057924976\n",
      "Iteration 1600 loss: 6.407694211365272\n",
      "Iteration 1700 loss: 10.09848296386325\n",
      "Iteration 1800 loss: 15.982810007887675\n",
      "Iteration 1900 loss: 25.36159593762312\n",
      "Iteration 2000 loss: 40.27917592509623\n",
      "Iteration 2100 loss: 63.920555113149184\n",
      "Iteration 2200 loss: 101.20202124238548\n",
      "Iteration 2300 loss: 159.63882752458358\n",
      "Iteration 2400 loss: 250.60524745502678\n",
      "Iteration 2500 loss: 391.1441612847313\n",
      "Iteration 2600 loss: 606.5383833387142\n",
      "Iteration 2700 loss: 933.9278988036803\n",
      "Iteration 2800 loss: 1427.3508133048467\n",
      "Iteration 2900 loss: 2164.707289140402\n",
      "Iteration 3000 loss: 3257.3030927432915\n",
      "Iteration 3100 loss: 4862.833223394846\n",
      "Iteration 3200 loss: 7202.930584598613\n",
      "Iteration 3300 loss: 10586.748843954148\n",
      "Iteration 3400 loss: 15442.498358002487\n",
      "Iteration 3500 loss: 22359.444654331677\n",
      "Iteration 3600 loss: 32143.659043074702\n",
      "Iteration 3700 loss: 45891.84754508756\n",
      "Iteration 3800 loss: 65088.97038214047\n",
      "Iteration 3900 loss: 91737.22878502312\n",
      "Iteration 4000 loss: 128526.51830509363\n",
      "Iteration 4100 loss: 179059.878176246\n",
      "Iteration 4200 loss: 248152.1532739748\n",
      "Iteration 4300 loss: 342226.5157501796\n",
      "Iteration 4400 loss: 469842.3480640976\n",
      "Iteration 4500 loss: 642400.2202324654\n",
      "Iteration 4600 loss: 875086.6358217741\n",
      "Iteration 4700 loss: 1188144.7445284156\n",
      "Iteration 4800 loss: 1608589.9508184385\n",
      "Iteration 4900 loss: 2172534.978869053\n",
      "Iteration 5000 loss: 2928352.6737566115\n",
      "Iteration 5100 loss: 3940993.9282294046\n",
      "Iteration 5200 loss: 5297902.895301594\n",
      "Iteration 5300 loss: 7117146.528275204\n",
      "Iteration 5400 loss: 9558620.794800172\n",
      "Iteration 5500 loss: 12839540.170483334\n",
      "Iteration 5600 loss: 17255900.248860028\n",
      "Iteration 5700 loss: 23212281.54700793\n",
      "Iteration 5800 loss: 31263314.154792976\n",
      "Iteration 5900 loss: 42171456.96524137\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.005503490756860151\n",
      "Iteration 200 loss: 0.022610091791948755\n",
      "Iteration 300 loss: 0.053188795736625764\n",
      "Iteration 400 loss: 0.10064129438948737\n",
      "Iteration 500 loss: 0.1703993142666406\n",
      "Iteration 600 loss: 0.2707716043691785\n",
      "Iteration 700 loss: 0.41433280916784576\n",
      "Iteration 800 loss: 0.6201714850852577\n",
      "Iteration 900 loss: 0.9175137418819684\n",
      "Iteration 1000 loss: 1.3515583892430587\n",
      "Iteration 1100 loss: 1.9928703732394235\n",
      "Iteration 1200 loss: 2.952489963758633\n",
      "Iteration 1300 loss: 4.4061859015725355\n",
      "Iteration 1400 loss: 6.633243040643483\n",
      "Iteration 1500 loss: 10.07815550290868\n",
      "Iteration 1600 loss: 15.448045835544315\n",
      "Iteration 1700 loss: 23.86515949885968\n",
      "Iteration 1800 loss: 37.10320438994368\n",
      "Iteration 1900 loss: 57.94968139629083\n",
      "Iteration 2000 loss: 90.75506084652707\n",
      "Iteration 2100 loss: 142.25546213628647\n",
      "Iteration 2200 loss: 222.79062201273587\n",
      "Iteration 2300 loss: 348.0862012055936\n",
      "Iteration 2400 loss: 541.8324004320891\n",
      "Iteration 2500 loss: 839.3738364562385\n",
      "Iteration 2600 loss: 1292.9341674862496\n",
      "Iteration 2700 loss: 1978.9399449534312\n",
      "Iteration 2800 loss: 3008.190275254824\n",
      "Iteration 2900 loss: 4539.853080029539\n",
      "Iteration 3000 loss: 6800.569065631609\n",
      "Iteration 3100 loss: 10110.329016633017\n",
      "Iteration 3200 loss: 14917.282222284815\n",
      "Iteration 3300 loss: 21844.26461806117\n",
      "Iteration 3400 loss: 31750.645537280223\n",
      "Iteration 3500 loss: 45814.1365813436\n",
      "Iteration 3600 loss: 65638.55875455891\n",
      "Iteration 3700 loss: 93395.32449031173\n",
      "Iteration 3800 loss: 132008.69528259852\n",
      "Iteration 3900 loss: 185397.90861691703\n",
      "Iteration 4000 loss: 258793.2832949786\n",
      "Iteration 4100 loss: 359148.7576402967\n",
      "Iteration 4200 loss: 495680.4676580351\n",
      "Iteration 4300 loss: 680570.5879588095\n",
      "Iteration 4400 loss: 929888.639749416\n",
      "Iteration 4500 loss: 1264800.060670163\n",
      "Iteration 4600 loss: 1713155.7448011015\n",
      "Iteration 4700 loss: 2311588.8634404163\n",
      "Iteration 4800 loss: 3108289.836615345\n",
      "Iteration 4900 loss: 4166691.3617725205\n",
      "Iteration 5000 loss: 5570379.17689059\n",
      "Iteration 5100 loss: 7429659.409630011\n",
      "Iteration 5200 loss: 9890371.96141134\n",
      "Iteration 5300 loss: 13145758.066980693\n",
      "Iteration 5400 loss: 17452492.084579445\n",
      "Iteration 5500 loss: 23152404.791758604\n",
      "Iteration 5600 loss: 30702002.470170457\n",
      "Iteration 5700 loss: 40712684.487739496\n",
      "Iteration 5800 loss: 54005667.215727255\n",
      "Iteration 5900 loss: 71687151.81071201\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.011813901909227641\n",
      "Iteration 200 loss: 0.048379381150254565\n",
      "Iteration 300 loss: 0.11321499103342975\n",
      "Iteration 400 loss: 0.21270834328919247\n",
      "Iteration 500 loss: 0.3570223952412202\n",
      "Iteration 600 loss: 0.5616344317687588\n",
      "Iteration 700 loss: 0.8498491249853735\n",
      "Iteration 800 loss: 1.2568450080674745\n",
      "Iteration 900 loss: 1.8361592058682807\n",
      "Iteration 1000 loss: 2.6700670939910647\n",
      "Iteration 1100 loss: 3.8861934081852216\n",
      "Iteration 1200 loss: 5.684083167833003\n",
      "Iteration 1300 loss: 8.377635997176178\n",
      "Iteration 1400 loss: 12.462656701994659\n",
      "Iteration 1500 loss: 18.723847829989104\n",
      "Iteration 1600 loss: 28.403123137671457\n",
      "Iteration 1700 loss: 43.462176541192385\n",
      "Iteration 1800 loss: 66.98815627735208\n",
      "Iteration 1900 loss: 103.81384302201553\n",
      "Iteration 2000 loss: 161.45520294273166\n",
      "Iteration 2100 loss: 251.51250220579638\n",
      "Iteration 2200 loss: 391.7400174645392\n",
      "Iteration 2300 loss: 609.068382386948\n",
      "Iteration 2400 loss: 943.9685386352282\n",
      "Iteration 2500 loss: 1456.6842710196945\n",
      "Iteration 2600 loss: 2236.040279109667\n",
      "Iteration 2700 loss: 3411.7656863133325\n",
      "Iteration 2800 loss: 5171.572514926269\n",
      "Iteration 2900 loss: 7784.612088487967\n",
      "Iteration 3000 loss: 11633.421106749212\n",
      "Iteration 3100 loss: 17257.090594502693\n",
      "Iteration 3200 loss: 25409.180006342693\n",
      "Iteration 3300 loss: 37134.90060995776\n",
      "Iteration 3400 loss: 53873.365692519845\n",
      "Iteration 3500 loss: 77592.32746227429\n",
      "Iteration 3600 loss: 110964.89418658543\n",
      "Iteration 3700 loss: 157600.3827246099\n",
      "Iteration 3800 loss: 222344.89434974527\n",
      "Iteration 3900 loss: 311671.65229430975\n",
      "Iteration 4000 loss: 434186.94110748847\n",
      "Iteration 4100 loss: 601285.0922289066\n",
      "Iteration 4200 loss: 827995.9799682009\n",
      "Iteration 4300 loss: 1134081.7602780461\n",
      "Iteration 4400 loss: 1545457.2347152235\n",
      "Iteration 4500 loss: 2096031.7975854457\n",
      "Iteration 4600 loss: 2830102.530162403\n",
      "Iteration 4700 loss: 3805470.514111989\n",
      "Iteration 4800 loss: 5097509.769708546\n",
      "Iteration 4900 loss: 6804495.746462376\n",
      "Iteration 5000 loss: 9054605.34436275\n",
      "Iteration 5100 loss: 12015143.077652993\n",
      "Iteration 5200 loss: 15904741.995028233\n",
      "Iteration 5300 loss: 21009552.25426516\n",
      "Iteration 5400 loss: 27704790.780964807\n",
      "Iteration 5500 loss: 36483517.8783593\n",
      "Iteration 5600 loss: 47995179.970772766\n",
      "Iteration 5700 loss: 63097379.102680825\n",
      "Iteration 5800 loss: 82925591.6672561\n",
      "Iteration 5900 loss: 108987287.65866601\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.020505676879795404\n",
      "Iteration 200 loss: 0.08383326285390423\n",
      "Iteration 300 loss: 0.19564852707659455\n",
      "Iteration 400 loss: 0.36622609818698265\n",
      "Iteration 500 loss: 0.6118876884458306\n",
      "Iteration 600 loss: 0.9574393052741036\n",
      "Iteration 700 loss: 1.4401400427904356\n",
      "Iteration 800 loss: 2.116071521262441\n",
      "Iteration 900 loss: 3.0703088055674037\n",
      "Iteration 1000 loss: 4.433141004392717\n",
      "Iteration 1100 loss: 6.405938147235694\n",
      "Iteration 1200 loss: 9.302389951338577\n",
      "Iteration 1300 loss: 13.614163237591745\n",
      "Iteration 1400 loss: 20.115129241021375\n",
      "Iteration 1500 loss: 30.026030039166407\n",
      "Iteration 1600 loss: 45.27292611774694\n",
      "Iteration 1700 loss: 68.88953409086162\n",
      "Iteration 1800 loss: 105.63766567011321\n",
      "Iteration 1900 loss: 162.9540808147977\n",
      "Iteration 2000 loss: 252.3796022137102\n",
      "Iteration 2100 loss: 391.6916753216795\n",
      "Iteration 2200 loss: 608.0502075977961\n",
      "Iteration 2300 loss: 942.5853710686459\n",
      "Iteration 2400 loss: 1457.0136620644437\n",
      "Iteration 2500 loss: 2243.0754649750975\n",
      "Iteration 2600 loss: 3435.856718208961\n",
      "Iteration 2700 loss: 5232.40512288338\n",
      "Iteration 2800 loss: 7917.497532319177\n",
      "Iteration 2900 loss: 11898.984314515676\n",
      "Iteration 3000 loss: 17755.859216096072\n",
      "Iteration 3100 loss: 26303.117957003862\n",
      "Iteration 3200 loss: 38678.6239367722\n",
      "Iteration 3300 loss: 56458.656819643846\n",
      "Iteration 3400 loss: 81810.65882372129\n",
      "Iteration 3500 loss: 117694.01729712357\n",
      "Iteration 3600 loss: 168122.6653391541\n",
      "Iteration 3700 loss: 238507.0222479817\n",
      "Iteration 3800 loss: 336097.56758358044\n",
      "Iteration 3900 loss: 470558.459817201\n",
      "Iteration 4000 loss: 654707.4917426222\n",
      "Iteration 4100 loss: 905468.8819420742\n",
      "Iteration 4200 loss: 1245098.6902044707\n",
      "Iteration 4300 loss: 1702760.0327078882\n",
      "Iteration 4400 loss: 2316548.13296152\n",
      "Iteration 4500 loss: 3136095.430978312\n",
      "Iteration 4600 loss: 4225926.991905678\n",
      "Iteration 4700 loss: 5669789.696543135\n",
      "Iteration 4800 loss: 7576249.750098041\n",
      "Iteration 4900 loss: 10085948.132938612\n",
      "Iteration 5000 loss: 13381031.176173082\n",
      "Iteration 5100 loss: 17697444.93229834\n",
      "Iteration 5200 loss: 23341012.996152263\n",
      "Iteration 5300 loss: 30708529.090128582\n",
      "Iteration 5400 loss: 40315516.88395624\n",
      "Iteration 5500 loss: 52832879.43028539\n",
      "Iteration 5600 loss: 69135432.75066692\n",
      "Iteration 5700 loss: 90366365.39183189\n",
      "Iteration 5800 loss: 118023087.50937936\n",
      "Iteration 5900 loss: 154071864.50910315\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.03157881566856325\n",
      "Iteration 200 loss: 0.128971736902897\n",
      "Iteration 300 loss: 0.3004894038661186\n",
      "Iteration 400 loss: 0.561194559082855\n",
      "Iteration 500 loss: 0.9349951938804669\n",
      "Iteration 600 loss: 1.4581862248852069\n",
      "Iteration 700 loss: 2.1852055625830213\n",
      "Iteration 800 loss: 3.1978510246701446\n",
      "Iteration 900 loss: 4.619962540979318\n",
      "Iteration 1000 loss: 6.640780120447984\n",
      "Iteration 1100 loss: 9.552104590390806\n",
      "Iteration 1200 loss: 13.807410314275316\n",
      "Iteration 1300 loss: 20.11576762281916\n",
      "Iteration 1400 loss: 29.59066065772354\n",
      "Iteration 1500 loss: 43.98470213044045\n",
      "Iteration 1600 loss: 66.05745477577048\n",
      "Iteration 1700 loss: 100.14723214786699\n",
      "Iteration 1800 loss: 153.05173256822636\n",
      "Iteration 1900 loss: 235.37039477463625\n",
      "Iteration 2000 loss: 363.52825865946136\n",
      "Iteration 2100 loss: 562.7929814839332\n",
      "Iteration 2200 loss: 871.7211924125002\n",
      "Iteration 2300 loss: 1348.6371672506787\n",
      "Iteration 2400 loss: 2080.9677707197197\n",
      "Iteration 2500 loss: 3198.547418322422\n",
      "Iteration 2600 loss: 4892.383484784088\n",
      "Iteration 2700 loss: 7440.858254663505\n",
      "Iteration 2800 loss: 11245.965327433456\n",
      "Iteration 2900 loss: 16882.969758112562\n",
      "Iteration 3000 loss: 25167.883393672048\n",
      "Iteration 3100 loss: 37248.41110413626\n",
      "Iteration 3200 loss: 54725.61401357306\n",
      "Iteration 3300 loss: 79815.533247119\n",
      "Iteration 3400 loss: 115562.5249308839\n",
      "Iteration 3500 loss: 166119.20608589062\n",
      "Iteration 3600 loss: 237111.8722122639\n",
      "Iteration 3700 loss: 336115.2430604256\n",
      "Iteration 3800 loss: 473266.7149841023\n",
      "Iteration 3900 loss: 662058.3311855887\n",
      "Iteration 4000 loss: 920354.9352003759\n",
      "Iteration 4100 loss: 1271700.1267797945\n",
      "Iteration 4200 loss: 1746988.5983668386\n",
      "Iteration 4300 loss: 2386605.405248324\n",
      "Iteration 4400 loss: 3243161.33448829\n",
      "Iteration 4500 loss: 4384990.9608487375\n",
      "Iteration 4600 loss: 5900629.130030904\n",
      "Iteration 4700 loss: 7904546.410733819\n",
      "Iteration 4800 loss: 10544509.777783772\n",
      "Iteration 4900 loss: 14011048.521201191\n",
      "Iteration 5000 loss: 18549656.672321547\n",
      "Iteration 5100 loss: 24476564.97356601\n",
      "Iteration 5200 loss: 32199184.964783378\n",
      "Iteration 5300 loss: 42242688.57457082\n",
      "Iteration 5400 loss: 55284670.3935537\n",
      "Iteration 5500 loss: 72200489.44753672\n",
      "Iteration 5600 loss: 94122760.8098525\n",
      "Iteration 5700 loss: 122519643.35519208\n",
      "Iteration 5800 loss: 159298154.74209648\n",
      "Iteration 5900 loss: 206940882.36202267\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.04503331827553127\n",
      "Iteration 200 loss: 0.18379480329723266\n",
      "Iteration 300 loss: 0.4277376214020008\n",
      "Iteration 400 loss: 0.7976137259768094\n",
      "Iteration 500 loss: 1.3263449115451276\n",
      "Iteration 600 loss: 2.0638751906020665\n",
      "Iteration 700 loss: 3.0850456843631373\n",
      "Iteration 800 loss: 4.502183518290588\n",
      "Iteration 900 loss: 6.485120412104027\n",
      "Iteration 1000 loss: 9.292984442156857\n",
      "Iteration 1100 loss: 13.324692737650546\n",
      "Iteration 1200 loss: 19.19914425664321\n",
      "Iteration 1300 loss: 27.88244915285842\n",
      "Iteration 1400 loss: 40.88925095210111\n",
      "Iteration 1500 loss: 60.59986410381115\n",
      "Iteration 1600 loss: 90.75670911174194\n",
      "Iteration 1700 loss: 137.23527071220826\n",
      "Iteration 1800 loss: 209.23035697169124\n",
      "Iteration 1900 loss: 321.06278490153073\n",
      "Iteration 2000 loss: 494.90117227998445\n",
      "Iteration 2100 loss: 764.816420692557\n",
      "Iteration 2200 loss: 1182.752971908653\n",
      "Iteration 2300 loss: 1827.2237709330486\n",
      "Iteration 2400 loss: 2815.8308646010623\n",
      "Iteration 2500 loss: 4323.100131061678\n",
      "Iteration 2600 loss: 6605.620578835078\n",
      "Iteration 2700 loss: 10037.125081653756\n",
      "Iteration 2800 loss: 15156.97590026916\n",
      "Iteration 2900 loss: 22736.56841927867\n",
      "Iteration 3000 loss: 33869.49363947719\n",
      "Iteration 3100 loss: 50092.97003590003\n",
      "Iteration 3200 loss: 73550.15023674536\n",
      "Iteration 3300 loss: 107205.52989238339\n",
      "Iteration 3400 loss: 155128.9640140079\n",
      "Iteration 3500 loss: 222867.89382857567\n",
      "Iteration 3600 loss: 317932.51480591483\n",
      "Iteration 3700 loss: 450425.0451619419\n",
      "Iteration 3800 loss: 633852.3365513103\n",
      "Iteration 3900 loss: 886171.2663994719\n",
      "Iteration 4000 loss: 1231129.2714807505\n",
      "Iteration 4100 loss: 1699978.8267420693\n",
      "Iteration 4200 loss: 2333665.704455304\n",
      "Iteration 4300 loss: 3185617.87789936\n",
      "Iteration 4400 loss: 4325296.839295539\n",
      "Iteration 4500 loss: 5842718.387196739\n",
      "Iteration 4600 loss: 7854208.944538085\n",
      "Iteration 4700 loss: 10509740.656684052\n",
      "Iteration 4800 loss: 14002289.852765772\n",
      "Iteration 4900 loss: 18579796.911250103\n",
      "Iteration 5000 loss: 24560481.83280811\n",
      "Iteration 5100 loss: 32352503.20145595\n",
      "Iteration 5200 loss: 42479257.90092149\n",
      "Iteration 5300 loss: 55612030.70759189\n",
      "Iteration 5400 loss: 72612251.30975695\n",
      "Iteration 5500 loss: 94586347.93011315\n",
      "Iteration 5600 loss: 122957164.1483297\n",
      "Iteration 5700 loss: 159557212.99276176\n",
      "Iteration 5800 loss: 206750793.3654078\n",
      "Iteration 5900 loss: 267594341.21742517\n",
      "Iteration 0 loss: 0.0\n",
      "Iteration 100 loss: 0.060869184700698985\n",
      "Iteration 200 loss: 0.24830246203691045\n",
      "Iteration 300 loss: 0.5773931796842403\n",
      "Iteration 400 loss: 1.0754835988688407\n",
      "Iteration 500 loss: 1.7859368414398062\n",
      "Iteration 600 loss: 2.7745062024246714\n",
      "Iteration 700 loss: 4.1396604081307515\n",
      "Iteration 800 loss: 6.0290690021237365\n",
      "Iteration 900 loss: 8.66578241894148\n",
      "Iteration 1000 loss: 12.389753969519303\n",
      "Iteration 1100 loss: 17.723702589014838\n",
      "Iteration 1200 loss: 25.47759177844213\n",
      "Iteration 1300 loss: 36.91420782770935\n",
      "Iteration 1400 loss: 54.0109001241539\n",
      "Iteration 1500 loss: 79.87151595927837\n",
      "Iteration 1600 loss: 119.37068912566127\n",
      "Iteration 1700 loss: 180.15364978388556\n",
      "Iteration 1800 loss: 274.17353888050826\n",
      "Iteration 1900 loss: 420.0312511954816\n",
      "Iteration 2000 loss: 646.4983430752802\n",
      "Iteration 2100 loss: 997.7619929475516\n",
      "Iteration 2200 loss: 1541.1455460862578\n",
      "Iteration 2300 loss: 2378.3451821157614\n",
      "Iteration 2400 loss: 3661.602943708481\n",
      "Iteration 2500 loss: 5616.733603192886\n",
      "Iteration 2600 loss: 8575.568000361945\n",
      "Iteration 2700 loss: 13021.205603854152\n",
      "Iteration 2800 loss: 19650.52925082632\n",
      "Iteration 2900 loss: 29459.78029801403\n",
      "Iteration 3000 loss: 43860.68995351157\n",
      "Iteration 3100 loss: 64836.79475229527\n",
      "Iteration 3200 loss: 95152.23260628921\n",
      "Iteration 3300 loss: 138628.6467554372\n",
      "Iteration 3400 loss: 200509.97607309357\n",
      "Iteration 3500 loss: 287940.08052517916\n",
      "Iteration 3600 loss: 410584.59312010766\n",
      "Iteration 3700 loss: 581436.4285525313\n",
      "Iteration 3800 loss: 817854.4322852057\n",
      "Iteration 3900 loss: 1142897.2654588516\n",
      "Iteration 4000 loss: 1587030.500583748\n",
      "Iteration 4100 loss: 2190304.9818288987\n",
      "Iteration 4200 loss: 3005130.0084698712\n",
      "Iteration 4300 loss: 4099797.450660997\n",
      "Iteration 4400 loss: 5562954.64738327\n",
      "Iteration 4500 loss: 7509277.710022317\n",
      "Iteration 4600 loss: 10086666.435427228\n",
      "Iteration 4700 loss: 13485372.434393845\n",
      "Iteration 4800 loss: 17949589.975044046\n",
      "Iteration 4900 loss: 23792193.30308537\n",
      "Iteration 5000 loss: 31413506.657632813\n",
      "Iteration 5100 loss: 41325259.615968205\n",
      "Iteration 5200 loss: 54181231.80456667\n",
      "Iteration 5300 loss: 70816555.48919183\n",
      "Iteration 5400 loss: 92298259.63256618\n",
      "Iteration 5500 loss: 119990454.87801476\n",
      "Iteration 5600 loss: 155638642.76609865\n",
      "Iteration 5700 loss: 201479074.30454102\n",
      "Iteration 5800 loss: 260381003.3793134\n",
      "Iteration 5900 loss: 336032241.0753105\n"
=======
      "Loss at   200 iteration:  175.401\n",
      "Loss at   400 iteration:  159.912\n",
      "Loss at   600 iteration:  146.534\n",
      "Loss at   800 iteration:  134.941\n",
      "Loss at  1000 iteration:  124.859\n",
      "Loss at  1200 iteration:  116.056\n",
      "Loss at  1400 iteration:  108.339\n",
      "Loss at  1600 iteration:  101.542\n",
      "Loss at  1800 iteration:  95.529\n",
      "Loss at  2000 iteration:  90.184\n",
      "Loss at  2200 iteration:  85.409\n"
>>>>>>> CtrlCCtrlV
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
<<<<<<< HEAD
    "w = np.random.rand(4)\n",
    "\n",
    "EPOCHES = 8\n",
    "LEARNING_RATE = 1e-6\n",
    "BATCH_SIZE = 1\n",
    "ITERATIONS = int(SAMPLE_COUNT / BATCH_SIZE)\n",
=======
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, N_SAMPLES, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "np.random.seed(0)\n",
    "w = torch.randn(8, device = device, dtype = dtype)\n",
>>>>>>> CtrlCCtrlV
    "\n",
    "for t in range(4000):\n",
    "    y_pred = w[0] + w[1]*x + w[2]*x**2 + w[3]*x**3 + w[4]*x**4 + w[5]*x**5 + w[6]*x**6 + w[7]*x**7\n",
    "\n",
<<<<<<< HEAD
    "        y_predicted = deque(map(\n",
    "            lambda sample : (w[0]*sample + w[1]*(sample**3) + w[2]*(sample**5) + w[3]*(sample**7)).sum(),\n",
    "            batch\n",
    "        ))\n",
    "\n",
    "        predict_error = y_predicted - y[iteration*BATCH_SIZE : (iteration+1)*BATCH_SIZE]\n",
    "        loss = np.square(predict_error).mean()\n",
    "\n",
    "        gradients = np.array((\n",
    "            2*predict_error * batch ** 1,\n",
    "            2*predict_error * batch ** 3,\n",
    "            2*predict_error * batch ** 5,\n",
    "            2*predict_error * batch ** 7)).mean()\n",
    "\n",
    "        w += gradients*LEARNING_RATE\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration} loss: {loss}\")\n",
    "\n"
=======
    "    error = y_pred - y\n",
    "    loss = error.pow(2).mean().item()\n",
    "\n",
    "    gradients = torch.empty_like(w, device = device)\n",
    "    gradients[0] = (2 * error).mean(-1)\n",
    "    gradients[1] = (2 * error * x).mean(-1)\n",
    "    gradients[2] = (2 * error * (x**2)).mean(-1)\n",
    "    gradients[3] = (2 * error * (x**3)).mean(-1)\n",
    "    gradients[4] = (2 * error * (x**4)).mean(-1)\n",
    "    gradients[5] = (2 * error * (x**5)).mean(-1)\n",
    "    gradients[6] = (2 * error * (x**6)).mean(-1)\n",
    "    gradients[7] = (2 * error * (x**7)).mean(-1)\n",
    "\n",
    "    \"\"\" \n",
    "    ValueError: only tensor with size 1 can be converted to python scalar\n",
    "    gradients = torch.tensor((\n",
    "        2 * error,\n",
    "        2 * error * x,\n",
    "        2 * error * (x**2),\n",
    "        2 * error * (x**3),\n",
    "        2 * error * (x**4),\n",
    "        2 * error * (x**5),\n",
    "        2 * error * (x**6),\n",
    "        2 * error * (x**7)), device = device).mean(-1)\n",
    "    gradients = torch.from_numpy(__gradients, device = device)\n",
    "    \"\"\"\n",
    "\n",
    "    w -= gradients * LEARNING_RATE\n",
    "    \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")\n"
>>>>>>> CtrlCCtrlV
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at   200 iteration:  86.842\n",
      "Loss at   400 iteration:  77.675\n",
      "Loss at   600 iteration:  69.847\n",
      "Loss at   800 iteration:  63.148\n",
      "Loss at  1000 iteration:  57.402\n",
      "Loss at  1200 iteration:  52.459\n",
      "Loss at  1400 iteration:  48.195\n",
      "Loss at  1600 iteration:  44.505\n",
      "Loss at  1800 iteration:  41.300\n",
      "Loss at  2000 iteration:  38.506\n",
      "Loss at  2200 iteration:  36.060\n",
      "Loss at  2400 iteration:  33.910\n",
      "Loss at  2600 iteration:  32.012\n",
      "Loss at  2800 iteration:  30.327\n",
      "Loss at  3000 iteration:  28.824\n",
      "Loss at  3200 iteration:  27.478\n",
      "Loss at  3400 iteration:  26.265\n",
      "Loss at  3600 iteration:  25.167\n",
      "Loss at  3800 iteration:  24.167\n",
      "Loss at  4000 iteration:  23.253\n",
      "Loss at  4200 iteration:  22.413\n",
      "Loss at  4400 iteration:  21.636\n",
      "Loss at  4600 iteration:  20.916\n",
      "Loss at  4800 iteration:  20.245\n",
      "Loss at  5000 iteration:  19.618\n",
      "Loss at  5200 iteration:  19.028\n",
      "Loss at  5400 iteration:  18.473\n",
      "Loss at  5600 iteration:  17.948\n",
      "Loss at  5800 iteration:  17.450\n",
      "Loss at  6000 iteration:  16.977\n",
      "Loss at  6200 iteration:  16.526\n",
      "Loss at  6400 iteration:  16.096\n",
      "Loss at  6600 iteration:  15.684\n",
      "Loss at  6800 iteration:  15.290\n",
      "Loss at  7000 iteration:  14.911\n",
      "Loss at  7200 iteration:  14.547\n",
      "Loss at  7400 iteration:  14.197\n",
      "Loss at  7600 iteration:  13.860\n",
      "Loss at  7800 iteration:  13.535\n",
      "Loss at  8000 iteration:  13.222\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "N_SAMPLES = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, N_SAMPLES, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "torch.manual_seed(0)\n",
    "w = torch.randn(8, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "for t in range(4000):\n",
    "    y_pred = w[0] + w[1]*x + w[2]*x**2 + w[3]*x** 3 + w[4]*x**4 + w[5]*x**5 + w[6]*x**6 + w[7]*x**7\n",
    "\n",
    "    error = y_pred - y\n",
    "    loss = error.pow(2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * LEARNING_RATE\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad[:] = 0\n",
    "        \n",
    "    if t % 200 == 199:\n",
    "        print(f\"Loss at {t+1:> 5} iteration: {loss:> 5.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
