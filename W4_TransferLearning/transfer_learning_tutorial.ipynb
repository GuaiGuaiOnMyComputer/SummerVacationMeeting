{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transfer Learning for Computer Vision Tutorial\n",
        "\n",
        "Adapted from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "\n",
        "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)\n",
        "\n",
        "In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at [cs231n notes](https://cs231n.github.io/transfer-learning/)_ \n",
        "Quoting these notes,\n",
        "\n",
        "    In practice, very few people train an entire Convolutional Network\n",
        "    from scratch (with random initialization), because it is relatively\n",
        "    rare to have a dataset of sufficient size. Instead, it is common to\n",
        "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "    contains 1.2 million images with 1000 categories), and then use the\n",
        "    ConvNet either as an initialization or a fixed feature extractor for\n",
        "    the task of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resnet18 for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Resnet18\n",
        "\n",
        "A ```resnet18``` is a pretrained deep feed forward neural network with 17 convolution layers and 1 fully connected layer. According to [this thesis](ResnetRef.pdf), introducing feed forward shortcuts to a deeply stacked network can further increase the accuracy of the model. Such feed forward shortcuts can counter the problem with gradient vanishing by adding the input of the layer block to its output. A layer block is a pair of convolution layers in different sizes. During training, if the convolution layers in the layer block is not helpful, its weights can be lowered and thus the output of the layer block can be more dependent to its input, essentially bypassing the convolution layers within.\n",
        "\n",
        "The original ```Resnet18``` is designed to classify 1000 different labels, therefor its final layer is a fully connected layer taking in the features from the previous convolution layer and outputs 1000 scores for each label. In this demo we only have to classify 2 different lables, bees or ants. The final fully connected layer should be modified to satisfy our needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![fff](img/Resnet18.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from os.path import join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the\n",
        "data.\n",
        "\n",
        "The problem we're going to solve today is to train a model to classify\n",
        "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
        "There are 75 validation images for each class. Usually, this is a very\n",
        "small dataset to generalize upon, if trained from scratch. Since we\n",
        "are using transfer learning, we should be able to generalize reasonably\n",
        "well.\n",
        "\n",
        "This dataset is a very small subset of imagenet.\n",
        "\n",
        ".. Note ::\n",
        "   Download the data from\n",
        "   [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "DATA_DIR:str = join('SummerVacationMeeting', 'W4_TransferLearning', 'hymenoptera_data')\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "data_transforms:dict[str, transforms.Compose] = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\"\"\" \n",
        "create dataset objects for training & validation datasets and store them into image_datasets dictionary\n",
        "torchvision.datasets.ImageFolder inherits torch.utils.data.Dataset class \n",
        "\"\"\"\n",
        "training_dataset = ImageFolder(\n",
        "    root = join(DATA_DIR, 'train'),\n",
        "    transform = data_transforms['train']\n",
        ")\n",
        "val_dataset = ImageFolder(\n",
        "    root = join(DATA_DIR, 'val'),\n",
        "    transform = data_transforms['val']\n",
        ")\n",
        "image_datasets: dict[str, ImageFolder] = {'train': training_dataset, 'val': val_dataset}\n",
        "\n",
        "\n",
        "\"\"\" create dataloader objects for training & validation dataloaders and store them into dataloaders dictionary \"\"\"\n",
        "train_dataloader = DataLoader(training_dataset, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    shuffle = True, \n",
        "    num_workers = 4)\n",
        "val_dataloader = DataLoader(val_dataset, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    shuffle = True, \n",
        "    num_workers = 4)\n",
        "dataloaders:dict[str, DataLoader] = {'train': train_dataloader, 'val': val_dataloader}\n",
        "\n",
        "dataset_sizes = {train_val: len(image_datasets[train_val]) for train_val in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train All Layers and Weights in Resnet18\n",
        "\n",
        "Pytorch provides a ```models.resnet18``` class and its pretrained weights. The following cell makes ```model_ft``` an instance of ```resnet18``` and replaces its pretrained final fully connected layer with our own custom fully connected layer that takes in the same number of inputs from the previous convolution layer and outputs the scores for the 2 labels. This new fully conneced layer is to be trained.\n",
        "\n",
        "![entire resnet to be trained](img/Resnet18_learn_all.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\yusin/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "model_ft = models.resnet18(weights = \"IMAGENET1K_V1\")\n",
        "\"\"\"\n",
        "The possible weight options for resnet18 are \"DEFAULT\", \"IMAGENET1K_V1\" or None\n",
        "If the weight argument is left blank or set to None, no weights for resent18 will be loaded\n",
        "According to https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L10 option \"DEFAULT\" and \"IMAGENET_1K_V!\" loades the same set of weights\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "Replace the last fully connected layer in Resnet18 that takes in model_ft.fc.in_features number of features and out put 1000 class scores with our own custom\n",
        "fully connected layer that takes in model_ft.fc.in_features features and outputs 2 class scores. \n",
        "\"\"\"\n",
        "model_ft.fc = nn.Linear(model_ft.fc.in_features, 2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and evaluate\n",
        "\n",
        "It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.\n",
        "\n",
        "The training loop is written in a [seperate file](train_and_test_model.py). Both the training loop provided by the tutorial and the denestified version are present. For some reason, models trained using the denestified version performs significantly worse than the original versoin?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# FIXME: the train_model function in the tutorial performs better in terms of accuracy\n",
        "# from train_and_test_model import my_train_model\n",
        "# model_ft = my_train_model(model_ft, criterion, dataloaders, dataset_sizes, optimizer_ft, exp_lr_scheduler, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 0.7319 Acc: 0.6107\n",
            "val Loss: 0.2766 Acc: 0.9085\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.3875 Acc: 0.8566\n",
            "val Loss: 0.2595 Acc: 0.9020\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.6814 Acc: 0.7582\n",
            "val Loss: 0.2168 Acc: 0.8954\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.4619 Acc: 0.7992\n",
            "val Loss: 0.4713 Acc: 0.7974\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.5333 Acc: 0.8033\n",
            "val Loss: 0.2376 Acc: 0.9150\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.5661 Acc: 0.8033\n",
            "val Loss: 0.3659 Acc: 0.8758\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.5943 Acc: 0.7869\n",
            "val Loss: 0.4601 Acc: 0.8693\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.5597 Acc: 0.8115\n",
            "val Loss: 0.2945 Acc: 0.8758\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.3350 Acc: 0.8648\n",
            "val Loss: 0.2994 Acc: 0.9020\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.3644 Acc: 0.8607\n",
            "val Loss: 0.2553 Acc: 0.9150\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.3176 Acc: 0.8648\n",
            "val Loss: 0.2704 Acc: 0.9020\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.2673 Acc: 0.8730\n",
            "val Loss: 0.2956 Acc: 0.8889\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.2898 Acc: 0.8811\n",
            "val Loss: 0.2290 Acc: 0.9216\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.3351 Acc: 0.8484\n",
            "val Loss: 0.2428 Acc: 0.9020\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.2635 Acc: 0.8934\n",
            "val Loss: 0.2357 Acc: 0.9150\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.2580 Acc: 0.8811\n",
            "val Loss: 0.2242 Acc: 0.9150\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.2167 Acc: 0.9098\n",
            "val Loss: 0.2455 Acc: 0.9085\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.2196 Acc: 0.8934\n",
            "val Loss: 0.2449 Acc: 0.9020\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.2762 Acc: 0.9016\n"
          ]
        }
      ],
      "source": [
        "from train_and_test_model import tutorial_train_model\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "model_ft = tutorial_train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resnet as Fixed Feature Extractor\n",
        "\n",
        "Retraining all the layers, including those that were pretrained in ```resnet18``` might be unnecessary. The first 17 layers in this network are convolution layers. These convolution layers act like feature extractors, or filters, operating on given images. Their parameters and weights might have been optimized when we load them and don't require futher training.\n",
        "\n",
        "What requires training is the custom fully connected layer we have just added to this network. To train only the final layer and not train the others, we need to freeze all the network except the final one. We need to set ```requires_grad = False``` to freeze the parameters so that the gradients are not computed in ```backward()```. \n",
        "You can read more about this in the documentation\n",
        "[here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward)_.\n",
        "\n",
        "![only train the last layer resnet](img/Resnet18_learn_only_last_layer.svg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# FIXME: train_model function in the tutorial performs better in terms of accuracy\n",
        "# from train_and_test_model import my_train_model\n",
        "# model_conv = my_train_model(model_conv, criterion, dataloaders, dataset_sizes, optimizer_conv, exp_lr_scheduler, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train_and_test_model import tutorial_train_model\n",
        "model_conv = tutorial_train_model(model_conv, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes, DEVICE)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
