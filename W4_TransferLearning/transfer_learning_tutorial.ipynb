{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transfer Learning for Computer Vision Tutorial\n",
        "\n",
        "Adapted from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "\n",
        "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)\n",
        "\n",
        "In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at [cs231n notes](https://cs231n.github.io/transfer-learning/)_ \n",
        "Quoting these notes,\n",
        "\n",
        "    In practice, very few people train an entire Convolutional Network\n",
        "    from scratch (with random initialization), because it is relatively\n",
        "    rare to have a dataset of sufficient size. Instead, it is common to\n",
        "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "    contains 1.2 million images with 1000 categories), and then use the\n",
        "    ConvNet either as an initialization or a fixed feature extractor for\n",
        "    the task of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resnet18 for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Resnet18\n",
        "\n",
        "A ```resnet18``` is a pretrained deep feed forward neural network with 17 convolution layers and 1 fully connected layer. According to [this thesis](ResnetRef.pdf), introducing feed forward shortcuts to a deeply stacked network can further increase the accuracy of the model. Such feed forward shortcuts can counter the problem with gradient vanishing by adding the input of the layer block to its output. A layer block is a pair of convolution layers in different sizes. During training, if the convolution layers in the layer block is not helpful, its weights can be lowered and thus the output of the layer block can be more dependent to its input, essentially bypassing the convolution layers within.\n",
        "\n",
        "The original ```Resnet18``` is designed to classify 1000 different labels, therefor its final layer is a fully connected layer taking in the features from the previous convolution layer and outputs 1000 scores for each label. In this demo we only have to classify 2 different lables, bees or ants. The final fully connected layer should be modified to satisfy our needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![fff](img/Resnet18.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import join\n",
        "from PIL import Image\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the\n",
        "data.\n",
        "\n",
        "The problem we're going to solve today is to train a model to classify\n",
        "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
        "There are 75 validation images for each class. Usually, this is a very\n",
        "small dataset to generalize upon, if trained from scratch. Since we\n",
        "are using transfer learning, we should be able to generalize reasonably\n",
        "well.\n",
        "\n",
        "This dataset is a very small subset of imagenet.\n",
        "\n",
        ".. Note ::\n",
        "   Download the data from\n",
        "   [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "DATA_DIR:str = join('SummerVacationMeeting', 'W4_TransferLearning', 'hymenoptera_data')\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "data_transforms:dict[str, transforms.Compose] = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\"\"\" \n",
        "create dataset objects for training & validation datasets and store them into image_datasets dictionary\n",
        "torchvision.datasets.ImageFolder inherits torch.utils.data.Dataset class \n",
        "\"\"\"\n",
        "training_dataset = ImageFolder(\n",
        "    root = join(DATA_DIR, 'train'),\n",
        "    transform = data_transforms['train']\n",
        ")\n",
        "val_dataset = ImageFolder(\n",
        "    root = join(DATA_DIR, 'val'),\n",
        "    transform = data_transforms['val']\n",
        ")\n",
        "image_datasets: dict[str, ImageFolder] = {'train': training_dataset, 'val': val_dataset}\n",
        "\n",
        "\n",
        "\"\"\" create dataloader objects for training & validation dataloaders and store them into dataloaders dictionary \"\"\"\n",
        "train_dataloader = DataLoader(training_dataset, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    shuffle = True, \n",
        "    num_workers = 4)\n",
        "val_dataloader = DataLoader(val_dataset, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    shuffle = True, \n",
        "    num_workers = 4)\n",
        "dataloaders:dict[str, DataLoader] = {'train': train_dataloader, 'val': val_dataloader}\n",
        "\n",
        "dataset_sizes = {train_val: len(image_datasets[train_val]) for train_val in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train All Layers and Weights in Resnet18\n",
        "\n",
        "Pytorch provides a ```models.resnet18``` class and its pretrained weights. The following cell makes ```model_ft``` an instance of ```resnet18``` and replaces its pretrained final fully connected layer with our own custom fully connected layer that takes in the same number of inputs from the previous convolution layer and outputs the scores for the 2 labels. This new fully conneced layer is to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_ft = models.resnet18(weights = \"IMAGENET1K_V1\")\n",
        "\"\"\"\n",
        "The possible weight options for resnet18 are \"DEFAULT\", \"IMAGENET1K_V1\" or None\n",
        "If the weight argument is left blank or set to None, no weights for resent18 will be loaded\n",
        "According to https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L10 option \"DEFAULT\" and \"IMAGENET_1K_V!\" loades the same set of weights\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "Replace the last fully connected layer in Resnet18 that takes in model_ft.fc.in_features number of features and out put 1000 class scores with our own custom\n",
        "fully connected layer that takes in model_ft.fc.in_features features and outputs 2 class scores. \n",
        "\"\"\"\n",
        "model_ft.fc = nn.Linear(model_ft.fc.in_features, 2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and evaluate\n",
        "\n",
        "It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.\n",
        "\n",
        "The training loop is written in a [seperate file](train_and_test_model.py). Both the training loop provided by the tutorial and the denestified version are present. For some reason, models trained using the denestified version performs significantly worse than the original versoin?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# FIXME: the train_model function in the tutorial performs better in terms of accuracy\n",
        "# from train_and_test_model import my_train_model\n",
        "# model_ft = my_train_model(model_ft, criterion, dataloaders, dataset_sizes, optimizer_ft, exp_lr_scheduler, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train_and_test_model import tutorial_train_model\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft = tutorial_train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resnet as Fixed Feature Extractor\n",
        "\n",
        "Retraining all the layers, including those that were pretrained in ```resnet18``` might be unnecessary. The first 17 layers in this network are convolution layers. These convolution layers act like feature extractors, or filters, operating on given images. Their parameters and weights might have been optimized when we load them and don't require futher training.\n",
        "\n",
        "What requires training is the custom fully connected layer we have just added to this network. To train only the final layer and not train the others, we need to freeze all the network except the final one. We need to set ```requires_grad = False``` to freeze the parameters so that the gradients are not computed in ```backward()```. \n",
        "You can read more about this in the documentation\n",
        "[here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward)_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# FIXME: train_model function in the tutorial performs better in terms of accuracy\n",
        "from train_and_test_model import my_train_model\n",
        "model_conv = my_train_model(model_conv, criterion, dataloaders, dataset_sizes, optimizer_conv, exp_lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train_and_test_model import tutorial_train_model\n",
        "model_conv = tutorial_train_model(model_conv, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, dataset_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference on custom images\n",
        "\n",
        "Use the trained model to make predictions on custom images and visualize\n",
        "the predicted class labels along with the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_model_predictions(model,img_path):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    img = data_transforms['val'](img)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        ax = plt.subplot(2,2,1)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
        "        imshow(img.cpu().data[0])\n",
        "        \n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_model_predictions(\n",
        "    model_conv,\n",
        "    img_path='data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg'\n",
        ")\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Learning\n",
        "\n",
        "If you would like to learn more about the applications of transfer learning,\n",
        "checkout our [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html).\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
