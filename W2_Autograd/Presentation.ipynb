{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Autogradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "[Autograd Api Doc](https://pytorch.org/docs/stable/autograd.html)\n",
    "\n",
    "[Back Propagation](https://medium.com/ai-academy-taiwan/bacn-propagation-3946e8ed8c55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and partial differential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During neural networn tarining, gradient decent is a common way of determining how the weights within a neural networn should be adjusted. By calculating the parital derivatives of the loss function to a given weight, we can see the direction that a given weight should be adjusted. When the partial derivative reaches 0, the given weight is possibly at its optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following simple neural networn with 2 input features and 2 layers:\n",
    "\n",
    "<img src=\"img/SimpleNeuralNetwork.jpg\" width=\"900\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the inpact of $w^{1}_{11}$ on the loss function, calculate the partial derivative \n",
    "$\\frac{\\partial L}{\\partial w^{1}_{11}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\hat{y}:ground truth\\\\\n",
    "L = \\frac{1}{2n} \\sum_{k=1}^n (y - \\hat{y})^2, n=batch size\\\\\n",
    "\\text{Assume batch size n = 1, then}L = \\frac{1}{2}(y - \\hat{y})^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w^{1}_{11}} &= \\frac{\\partial L}{\\partial z^3_1} \\frac{\\partial z^3_1}{\\partial w^{1}_{11}}\\\\\n",
    "&= \\frac{\\partial L}{\\partial z^3_1} (\\frac{\\partial z^3_1}{\\partial a^2_1} \\frac{\\partial a^2_1}{\\partial z^2_1} \\frac{\\partial z^2_1}{\\partial a^1_1} \\frac{\\partial a^1_1}{\\partial z^1_1} \\frac{\\partial z^1_1}{\\partial w^1_{11}} + \\frac{\\partial z^3_1}{\\partial a^2_2} \\frac{\\partial a^2_2}{\\partial z^2_2} \\frac{\\partial z^2_2}{\\partial a^1_1} \\frac{\\partial a^1_1}{\\partial z^1_1} \\frac{\\partial z^1_1}{\\partial w^1_{11}}) \\\\\n",
    "&= \\frac{\\partial L}{\\partial z^3_1} (\\frac{\\partial z^3_1}{\\partial a^2_1} \\frac{\\partial a^2_1}{\\partial z^2_1} \\frac{\\partial z^2_1}{\\partial a^1_1} + \\frac{\\partial z^3_1}{\\partial a^2_2} \\frac{\\partial a^2_2}{\\partial z^2_2} \\frac{\\partial z^2_2}{\\partial a^1_1})*( \\frac{\\partial a^1_1}{\\partial z^1_1} \\frac{\\partial z^1_1}{\\partial w^1_{11}}) \\\\\n",
    "&= \\frac{\\partial L}{\\partial z^3_1} (\\sum_{n=1}^2\\frac{\\partial z^3_1}{\\partial a^n_1} \\frac{\\partial a^n_1}{\\partial z^n_1} \\frac{\\partial z^n_1}{\\partial a^1_1}) * ( \\frac{\\partial a^1_1}{\\partial z^1_1} \\frac{\\partial z^1_1}{\\partial w^1_{11}}) \\text{ since } y = z^3_1 \\text{ then } \\partial z^3_1 = \\partial y\\\\\n",
    "\n",
    "&= \\frac{\\partial L}{\\partial y} (\\sum_{n=1}^2\\frac{\\partial z^3_1}{\\partial a^n_1} \\frac{\\partial a^n_1}{\\partial z^n_1} \\frac{\\partial z^n_1}{\\partial a^1_1}) * ( \\frac{\\partial a^1_1}{\\partial z^1_1} \\frac{\\partial z^1_1}{\\partial w^1_{11}})\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{subsitude}\\\\\n",
    "&\\frac{\\partial L}{\\partial y} = \\frac{\\partial}{\\partial y}[\\frac{1}{2}(y-\\hat{y})^2] = y - \\hat{y}\\\\\n",
    "&\\frac{\\partial z^3_1}{\\partial a^2_1}=\\frac{\\partial}{\\partial a^2_1}{(w^3_{11} a^2_1 + w^3_{12} a^2_2)} = w^3_{11}\\\\\n",
    "&\\frac{\\partial a^n_1}{\\partial z^n_1} = \\frac{\\partial f(z^n_1)}{\\partial z^n_1}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w^{1}_{11}} = (y - \\hat{y}) \\sum_{n=1}^2(w^3_{11}\\frac{\\partial f(z^n_1)}{\\partial z^n_1} w^2_{11}) \\frac{\\partial f(z^1_1)}{\\partial z^1_1}\\frac{\\partial z^1_1}{\\partial w^1_{11}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take large amount of resourses to solve all the partial derivatives with brute force. However, these remaining partial derivatives are the partial derivative of the activation functions with respect to its input, which and usually have analytical solutions and can be easily calculated.\n",
    "\n",
    "Pytorch tensors can record the functions it has been passed into from the input layer $x$ all the way to the output layer $y$. After a tensor $x$ has been processed by multiple layers of neurons and weighted, generating an output tensor $y$, tensor $y$ will contain detailed information about all the process it has been through in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.arange(0, 5, 1, dtype = torch.float32, requires_grad = True)\n",
    "print(\"Initial x = \", x1)\n",
    "x1 = x1 + 1\n",
    "print(\"x + 1 = \", x1)\n",
    "x1 *= 2\n",
    "print(\"2*(x + 1)\", x1)\n",
    "x1 **= 2\n",
    "print(\"pow(2(x+1), 2)\", x1)\n",
    "x1 /= 2\n",
    "print(\"pow(2(x+1)/2, 2)\", x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual usage of autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are training this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "BATCH_SIZE = 2\n",
    "DIM_IN = 7\n",
    "HIDDEN_SIZE = 4\n",
    "DIM_OUT = 2\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n",
    "        # the grad_fn shows Addmmobject, which is what linear layer does\n",
    "        # Addmmobject ref: https://pytorch.org/docs/stable/generated/torch.addmm.html\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)     # random tensor as an input to the model\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)  # random ground truth \n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "model = TinyModel()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_output = model(some_input)\n",
    "loss = loss_fn(predicted_output, ideal_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, the model has predicted what ```some_input``` by its absurd and under-trained parameters. Also, the loss function has calculated how far the model output deviates from ```ideal_output```. But the gradients of the model have not been calculated yet. What that means is that the model have no idea how to adjust its weights to reduce loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current weights of model's layer1: (in {DIM_IN} out {HIDDEN_SIZE})\")\n",
    "model.layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current gradient of model's layer1:\")\n",
    "print(model.layer1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So let's calculate its weights using the ```tensor.backwards()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(\"Gradients of model's layer1 after loss.backward() method call:\")\n",
    "print(model.layer1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gradients known, we can call the ```optimizer.step``` method to adjust the weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "print(model.layer1.weight)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Although it is needed to calculate lots of partial differentiations during model training, the autograd feature provided by pytorch makes the process painless and simple. By logging all the mathematical operations a tensor has gone through from the input layer of the model all the way to its output layer and the loss function, the complicated partial derivatives can be found analytically.\n",
    "\n",
    "Calling the ```backward``` method on the result of the loss function tensor, the gradients of the model can be found and the optimizer can adjust the model weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
